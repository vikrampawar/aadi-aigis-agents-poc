Making them into a mesh -
28-Feb-26

Read the Easy Wins file currently opened and the design principles in there as a basis for any work we do. Commit them to memory so you use these every time. We have previously worked on Agent 01 (VDR Inventory) and Agent 04 (Financial Calculator). These have been designed as standalone tools/agents. However, I would like to now convert the whole project into an agent-mesh, where each agent is aware of and able to call every other agent in the system. What are the best practices for this? My thinking is below - 1. we prepare a tool-kit file, which contains the name of the agent, brief description, input parameters, dependencies, output format, and other information. This file should be updated regularly with any new agents / tools that we implement and kept live for easy reference by the agent; 2. Further, in the root folder of the repo, there is a domain_knowledge repository, that contains all the distilled mark down files for sector knowledge to provide context to LLM / agents. Every agent we build should be aware of this and able to call on it as required; 3. Every agent should have two output modes depending on the input parameters - if its just called directly without any input constraints, it should produce a structured, human readable output summary in an actionable mark down file. However, if it is tool-called directly from another agent for a specific task, it should just provide the specific output required as json. So let's design them for both options; 4. Every agent we build will have an LLM layer running alongside, which will review the inputs for quality and purpose, check the efficacy and reasonableness of the outputs being generated and generally audit the working of the agent; 5. Further, each agent should have persistent memory elements built in, and other agents should be able to suggest improvements / amendments to its outputs as required to be able to improve for future iterations. 

In planning mode, and for the above requirements, build out a spec and a framework for me to review. Save the spec file in the plan_docs folder as a mark down file.  

--------------------

Answering the questions - 1. For each individual agent, let's have the ability to provide the LLM selection as an input parameter (with API key provision functionality as required). So there will be two LLMs implemented in each agent - there will be the main LLM that runs or oversees the task, which can be selected via input parameters; and secondly there will be a cheaper LLM for the quality layer to audit inputs and outputs. 2. Let's keep the JSON for now, and we move to SQLite when the number of agents increase to warrant it. 3. For the moment, improvements should always be human reviewed. However, let's have a running list of suggestions + result of human review. Once you see that a majority of the suggestions are being approved as suggested, we can give the user the option to toggle auto application above a confidence threshold as it shows their confidence in the system's suggestions. 4. cache domain knowledge per session as its unlikely to materially change during the session. Rework the plan document for these changes and provide it for review.